{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6598d895",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67c2eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "web='https://technopark.org/job-search'\n",
    "path='C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "driver=webdriver.Chrome(path)\n",
    "driver.get(web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01b833",
   "metadata": {},
   "source": [
    "# Trial 1-Technopark job listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0604144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h4 class=\"bodyemphasis flex-shrink uppercase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46732f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = driver.find_elements(By.CLASS_NAME, \"bodyemphasis\")\n",
    "\n",
    "# Extract the text content of each element\n",
    "job_listings = [job.text for job in job_elements]\n",
    "\n",
    "job_firms = driver.find_elements(By.CLASS_NAME, \"text-xs\")\n",
    "\n",
    "job_firms2 = [job.text for job in job_firms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9d0ecdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'SENIOR SHAREPOINT DEVELOPER',\n",
       " 'BUSINESS DEVELOPMENT EXECUTIVE',\n",
       " 'SOLUTION ARCHITECT (DIGITAL/ DATA PRACTICE)',\n",
       " 'QA PRACTICE LEAD (HANDSON, DOMAIN SPECIALIST)',\n",
       " 'TECHNICAL PRODUCT MANAGER (WORK FROM HOME)',\n",
       " 'TRAINEES - BUSINESS GROWTH TEAM',\n",
       " 'BRAND DESIGN INTERNSHIP OPPORTUNITY AT TECHBAND TECHNOLOGY PRIVATE LIMITED',\n",
       " 'YOUTUBE EXECUTIVE',\n",
       " 'EXECUTIVE SECRETARY TO THE CEO',\n",
       " 'FLUTTER DEVELOPER',\n",
       " 'LINUX DEVOPS ENGINEER',\n",
       " 'SENIOR SOFTWARE ENGINEER- DOT NET',\n",
       " 'HR- FRESHER',\n",
       " 'AI/ML PLATFORM ARCHITECT',\n",
       " 'LEAD FULL STACK DEVELOPER (NODE JS + ANGULAR)',\n",
       " 'AI/ML ENGINEER',\n",
       " 'DOT NET TEAM LEAD',\n",
       " 'MACHINE LEARNING ENGINEER',\n",
       " 'COLDFUSION DEVELOPER',\n",
       " 'IMMUNOLOGY/INFLAMMATION EXPERT (POSTDOCTORAL FELLOW / SCIENTIST)',\n",
       " 'FRESHER DEVELOPER',\n",
       " 'NODE DEVELOPER',\n",
       " 'DIGITAL MARKETER JUNIOR',\n",
       " 'PRODUCT OWNER',\n",
       " 'ANGULAR DEVELOPER',\n",
       " 'BUSINESS DEVELOPMENT EXECUTIVE',\n",
       " 'ASP.NET DEVELOPER',\n",
       " 'SENIOR DOT NET DEVELOPER',\n",
       " 'PYTHON DJANGO + AWS DEVELOPER',\n",
       " 'SENIOR SQL SERVER DBA',\n",
       " 'ANDROID DEVELOPMENT FROM 3 TO 8YEARS OF EXPERIENCE',\n",
       " 'DIGITAL MARKETING INTERN',\n",
       " 'REVIEW OPERATIONS ASSISTANT',\n",
       " 'REACT DEVELOPER',\n",
       " 'IT BUSINESS ANALYST',\n",
       " '.NET DEVELOPER',\n",
       " 'BUSINESS ANALYST',\n",
       " 'ACCOUNTANT',\n",
       " 'PROGRAM MANAGER',\n",
       " 'LEAD ENGINEER- .NETCORE',\n",
       " 'WEB DEVELOPER (1-5 YEARS EXP)',\n",
       " 'BLUEPRINT DEVELOPER (UNREAL ENGINE)',\n",
       " 'UI/UX DESIGNER',\n",
       " 'TESTING - FRESHER (2023, 2024 & 2025 PASSOUTS CAN APPLY)',\n",
       " 'FRONT END DEVELOPER',\n",
       " 'TECHNICAL ARCHITECT',\n",
       " 'DIGITAL MEDIA CONTENT WRITER - URGENT REQUIREMENT',\n",
       " 'OPERATIONS ASSISTANT',\n",
       " 'IT PROJECT MANAGER (CUSTOMER SUCCESS)',\n",
       " 'JR. CLOUD DEVOPS ENGINEER',\n",
       " 'PROCUREMENT OFFICER - AUTOMOBILE MECHANICAL PARTS',\n",
       " 'CUSTOMER RELATIONSHIP EXECUTIVE',\n",
       " 'SENIOR ACCOUNTANT',\n",
       " 'SOFTWARE TEST ENGINEER(EXPERIENCED)',\n",
       " 'FRONT END DEVELOPER',\n",
       " 'SITE RELIABILITY ENGINEER (SRE)',\n",
       " 'FLUTTER DEVELOPER',\n",
       " 'SR. DOT NET DEVELOPER',\n",
       " 'SENIOR JAVA DEVELOPER (MUST HAVE 5+ YEARS OF EXPERIENCE) – IMMEDIATE JOINERS PREFERRED',\n",
       " 'OPERATIONS PERSONNEL',\n",
       " 'ADMINISTRATIVE ASSISTANT',\n",
       " '.NET FULL STACK DEVELOPER',\n",
       " '.NET DEVELOPER',\n",
       " 'DEVOPS ENGINEER - JOB CODE: DEO - 164',\n",
       " 'HR TRAINEE',\n",
       " 'TECHNICAL SUPPORT ENGINEER – SMS INDUSTRY',\n",
       " 'SOFTWARE SALES EXECUTIVE - JOB CODE: SAE-165',\n",
       " 'DATA SCIENTIST',\n",
       " 'LINUX TECHNICAL SUPPORT',\n",
       " 'PORTING ENGINEER',\n",
       " 'DATA ENGINEER',\n",
       " 'SENIOR 3D ARTIST',\n",
       " 'JUNIOR 3D ARTIST',\n",
       " 'SENIOR UNITY/UNREAL DEVELOPER',\n",
       " 'JUNIOR UNITY/UNREAL DEVELOPER',\n",
       " 'WORDPRESS DEVELOPER',\n",
       " 'SENIOR JAVA DEVELOPER',\n",
       " 'SENIOR PYTHON DEVELOPER',\n",
       " 'TECHNICAL ARCHITECT - JAVA',\n",
       " 'JAVA TEAM LEAD',\n",
       " 'Happening Here',\n",
       " 'Quick Links',\n",
       " 'Companies & Jobs',\n",
       " '\"A surprising innovation hotspot powered by human spirit and fuelled by a nurturing environment.\"']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15514a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Closing Date: 14,Mar 2025',\n",
       " 'Reflections Info Systems Pvt Ltd',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 21,Mar 2025',\n",
       " 'Richinnovations Technologies Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 14,Mar 2025',\n",
       " 'Reflections Info Systems Pvt Ltd',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 14,Mar 2025',\n",
       " 'Reflections Info Systems Pvt Ltd',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 15,Mar 2025',\n",
       " 'LEADER IT PRIVATE LIMITED',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 14,Mar 2025',\n",
       " 'Softnotions Technologies Private Limited',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 17,Mar 2025',\n",
       " 'Techband Technologies Private Limited',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 07,Apr 2025',\n",
       " 'Toonz Animation India PVT LTD',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 07,Apr 2025',\n",
       " 'Toonz Animation India PVT LTD',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 31,Mar 2025',\n",
       " 'INOVACE DIGITAL MEDIA LLP',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 31,Mar 2025',\n",
       " 'Feathersoft Info Solutions Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 13,Mar 2025',\n",
       " 'Techversant Infotech Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 05,Apr 2025',\n",
       " 'ECESIS CARE PVT LTD',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 15,Apr 2025',\n",
       " 'Feathersoft Info Solutions Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 13,Mar 2025',\n",
       " 'Techversant Infotech Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 15,Apr 2025',\n",
       " 'Feathersoft Info Solutions Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 13,Mar 2025',\n",
       " 'Techversant Infotech Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 15,Apr 2025',\n",
       " 'Feathersoft Info Solutions Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 13,Mar 2025',\n",
       " 'Techversant Infotech Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Closing Date: 15,Apr 2025',\n",
       " 'Feathersoft Info Solutions Pvt. Ltd.',\n",
       " 'Posted On: 07,Mar 2025',\n",
       " 'Official Site of TECHNOPARK . Copyright © technopark.org. 2024 Site by Xocortx']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_firms2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48df1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(job_firms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa8896d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Titles  \\\n",
      "0                                                       \n",
      "1                         SENIOR SHAREPOINT DEVELOPER   \n",
      "2                      BUSINESS DEVELOPMENT EXECUTIVE   \n",
      "3         SOLUTION ARCHITECT (DIGITAL/ DATA PRACTICE)   \n",
      "4       QA PRACTICE LEAD (HANDSON, DOMAIN SPECIALIST)   \n",
      "5          TECHNICAL PRODUCT MANAGER (WORK FROM HOME)   \n",
      "6                     TRAINEES - BUSINESS GROWTH TEAM   \n",
      "7   BRAND DESIGN INTERNSHIP OPPORTUNITY AT TECHBAN...   \n",
      "8                                   YOUTUBE EXECUTIVE   \n",
      "9                      EXECUTIVE SECRETARY TO THE CEO   \n",
      "10                                  FLUTTER DEVELOPER   \n",
      "11                              LINUX DEVOPS ENGINEER   \n",
      "12                  SENIOR SOFTWARE ENGINEER- DOT NET   \n",
      "13                                        HR- FRESHER   \n",
      "14                           AI/ML PLATFORM ARCHITECT   \n",
      "15      LEAD FULL STACK DEVELOPER (NODE JS + ANGULAR)   \n",
      "16                                     AI/ML ENGINEER   \n",
      "17                                  DOT NET TEAM LEAD   \n",
      "18                          MACHINE LEARNING ENGINEER   \n",
      "19                               COLDFUSION DEVELOPER   \n",
      "\n",
      "                                       Firms               Closing Date  \\\n",
      "0           Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
      "1     Richinnovations Technologies Pvt. Ltd.  Closing Date: 21,Mar 2025   \n",
      "2           Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
      "3           Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
      "4                  LEADER IT PRIVATE LIMITED  Closing Date: 15,Mar 2025   \n",
      "5   Softnotions Technologies Private Limited  Closing Date: 14,Mar 2025   \n",
      "6      Techband Technologies Private Limited  Closing Date: 17,Mar 2025   \n",
      "7              Toonz Animation India PVT LTD  Closing Date: 07,Apr 2025   \n",
      "8              Toonz Animation India PVT LTD  Closing Date: 07,Apr 2025   \n",
      "9                  INOVACE DIGITAL MEDIA LLP  Closing Date: 31,Mar 2025   \n",
      "10      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 31,Mar 2025   \n",
      "11            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "12                       ECESIS CARE PVT LTD  Closing Date: 05,Apr 2025   \n",
      "13      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "14            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "15      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "16            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "17      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "18            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "19      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "\n",
      "               Posted Date  \n",
      "0   Posted On: 07,Mar 2025  \n",
      "1   Posted On: 07,Mar 2025  \n",
      "2   Posted On: 07,Mar 2025  \n",
      "3   Posted On: 07,Mar 2025  \n",
      "4   Posted On: 07,Mar 2025  \n",
      "5   Posted On: 07,Mar 2025  \n",
      "6   Posted On: 07,Mar 2025  \n",
      "7   Posted On: 07,Mar 2025  \n",
      "8   Posted On: 07,Mar 2025  \n",
      "9   Posted On: 07,Mar 2025  \n",
      "10  Posted On: 07,Mar 2025  \n",
      "11  Posted On: 07,Mar 2025  \n",
      "12  Posted On: 07,Mar 2025  \n",
      "13  Posted On: 07,Mar 2025  \n",
      "14  Posted On: 07,Mar 2025  \n",
      "15  Posted On: 07,Mar 2025  \n",
      "16  Posted On: 07,Mar 2025  \n",
      "17  Posted On: 07,Mar 2025  \n",
      "18  Posted On: 07,Mar 2025  \n",
      "19  Posted On: 07,Mar 2025  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Techno Park job search page\n",
    "web = \"https://technopark.org/job-search\"\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(web)\n",
    "\n",
    "# Find all job titles and firm-related info\n",
    "job_elements = driver.find_elements(By.CLASS_NAME, \"bodyemphasis\")\n",
    "job_listings = [job.text for job in job_elements]\n",
    "\n",
    "job_firms = driver.find_elements(By.CLASS_NAME, \"text-xs\")\n",
    "job_firms2 = [job.text for job in job_firms]\n",
    "\n",
    "# Group firm-related info into firm name, closing date, and posted date\n",
    "firms = job_firms2[1::3]          # Every 3rd item starting from the second (firm names)\n",
    "closing_dates = job_firms2[0::3] # Every 3rd item starting from the first (closing dates)\n",
    "posted_dates = job_firms2[2::3]  # Every 3rd item starting from the third (posted dates)\n",
    "\n",
    "# Ensure all lists have the same length\n",
    "min_length = min(len(job_listings), len(firms), len(closing_dates), len(posted_dates))\n",
    "\n",
    "# Create a DataFrame from job titles, firm names, closing dates, and posted dates\n",
    "df = pd.DataFrame({\n",
    "    \"Job Titles\": job_listings[:min_length],\n",
    "    \"Firms\": firms[:min_length],\n",
    "    \"Closing Date\": closing_dates[:min_length],\n",
    "    \"Posted Date\": posted_dates[:min_length]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ebd628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Titles  \\\n",
      "0                                                       \n",
      "1                         SENIOR SHAREPOINT DEVELOPER   \n",
      "2                      BUSINESS DEVELOPMENT EXECUTIVE   \n",
      "3         SOLUTION ARCHITECT (DIGITAL/ DATA PRACTICE)   \n",
      "4       QA PRACTICE LEAD (HANDSON, DOMAIN SPECIALIST)   \n",
      "5          TECHNICAL PRODUCT MANAGER (WORK FROM HOME)   \n",
      "6                     TRAINEES - BUSINESS GROWTH TEAM   \n",
      "7   BRAND DESIGN INTERNSHIP OPPORTUNITY AT TECHBAN...   \n",
      "8                                   YOUTUBE EXECUTIVE   \n",
      "9                      EXECUTIVE SECRETARY TO THE CEO   \n",
      "10                                  FLUTTER DEVELOPER   \n",
      "11                              LINUX DEVOPS ENGINEER   \n",
      "12                  SENIOR SOFTWARE ENGINEER- DOT NET   \n",
      "13                                        HR- FRESHER   \n",
      "14                           AI/ML PLATFORM ARCHITECT   \n",
      "15      LEAD FULL STACK DEVELOPER (NODE JS + ANGULAR)   \n",
      "16                                     AI/ML ENGINEER   \n",
      "17                                  DOT NET TEAM LEAD   \n",
      "18                          MACHINE LEARNING ENGINEER   \n",
      "19                               COLDFUSION DEVELOPER   \n",
      "\n",
      "                                       Firms               Closing Date  \\\n",
      "0           Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
      "1     Richinnovations Technologies Pvt. Ltd.  Closing Date: 21,Mar 2025   \n",
      "2           Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
      "3           Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
      "4                  LEADER IT PRIVATE LIMITED  Closing Date: 15,Mar 2025   \n",
      "5   Softnotions Technologies Private Limited  Closing Date: 14,Mar 2025   \n",
      "6      Techband Technologies Private Limited  Closing Date: 17,Mar 2025   \n",
      "7              Toonz Animation India PVT LTD  Closing Date: 07,Apr 2025   \n",
      "8              Toonz Animation India PVT LTD  Closing Date: 07,Apr 2025   \n",
      "9                  INOVACE DIGITAL MEDIA LLP  Closing Date: 31,Mar 2025   \n",
      "10      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 31,Mar 2025   \n",
      "11            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "12                       ECESIS CARE PVT LTD  Closing Date: 05,Apr 2025   \n",
      "13      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "14            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "15      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "16            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "17      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "18            Techversant Infotech Pvt. Ltd.  Closing Date: 13,Mar 2025   \n",
      "19      Feathersoft Info Solutions Pvt. Ltd.  Closing Date: 15,Apr 2025   \n",
      "\n",
      "   Posted Date  \n",
      "0   2025-03-07  \n",
      "1   2025-03-07  \n",
      "2   2025-03-07  \n",
      "3   2025-03-07  \n",
      "4   2025-03-07  \n",
      "5   2025-03-07  \n",
      "6   2025-03-07  \n",
      "7   2025-03-07  \n",
      "8   2025-03-07  \n",
      "9   2025-03-07  \n",
      "10  2025-03-07  \n",
      "11  2025-03-07  \n",
      "12  2025-03-07  \n",
      "13  2025-03-07  \n",
      "14  2025-03-07  \n",
      "15  2025-03-07  \n",
      "16  2025-03-07  \n",
      "17  2025-03-07  \n",
      "18  2025-03-07  \n",
      "19  2025-03-07  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Techno Park job search page\n",
    "web = \"https://technopark.org/job-search\"\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(web)\n",
    "\n",
    "# Define cutoff date for filtering\n",
    "cutoff_date = \"2025-02-15\"\n",
    "\n",
    "# Find all job titles and firm-related info\n",
    "job_elements = driver.find_elements(By.CLASS_NAME, \"bodyemphasis\")\n",
    "job_listings = [job.text for job in job_elements]\n",
    "\n",
    "job_firms = driver.find_elements(By.CLASS_NAME, \"text-xs\")\n",
    "job_firms2 = [job.text for job in job_firms]\n",
    "\n",
    "# Group firm-related info into firm name, closing date, and posted date\n",
    "firms = job_firms2[1::3]          # Every 3rd item starting from the second (firm names)\n",
    "closing_dates = job_firms2[0::3] # Every 3rd item starting from the first (closing dates)\n",
    "posted_dates = job_firms2[2::3]  # Every 3rd item starting from the third (posted dates)\n",
    "\n",
    "# Clean up posted_dates\n",
    "clean_posted_dates = [date.replace(\"Posted On: \", \"\").strip() for date in posted_dates]\n",
    "\n",
    "# Convert to datetime for comparison\n",
    "clean_posted_dates = pd.to_datetime(clean_posted_dates, format=\"%d,%b %Y\", errors='coerce')\n",
    "\n",
    "# Ensure all lists have the same length\n",
    "min_length = min(len(job_listings), len(firms), len(closing_dates), len(clean_posted_dates))\n",
    "\n",
    "# Filter based on posted date before creating the DataFrame\n",
    "filtered_jobs = []\n",
    "filtered_firms = []\n",
    "filtered_closing_dates = []\n",
    "filtered_posted_dates = []\n",
    "\n",
    "for i in range(min_length):\n",
    "    if clean_posted_dates[i] >= pd.to_datetime(cutoff_date):\n",
    "        filtered_jobs.append(job_listings[i])\n",
    "        filtered_firms.append(firms[i])\n",
    "        filtered_closing_dates.append(closing_dates[i])\n",
    "        filtered_posted_dates.append(clean_posted_dates[i])\n",
    "\n",
    "# Create a DataFrame from filtered data\n",
    "df = pd.DataFrame({\n",
    "    \"Job Titles\": filtered_jobs,\n",
    "    \"Firms\": filtered_firms,\n",
    "    \"Closing Date\": filtered_closing_dates,\n",
    "    \"Posted Date\": filtered_posted_dates\n",
    "})\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally save to CSV\n",
    "#df.to_csv(\"technopark_job_listings.csv\", index=False)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6ce7916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Firms</th>\n",
       "      <th>Closing Date</th>\n",
       "      <th>Posted Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Reflections Info Systems Pvt Ltd</td>\n",
       "      <td>Closing Date: 14,Mar 2025</td>\n",
       "      <td>2025-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SENIOR SHAREPOINT DEVELOPER</td>\n",
       "      <td>Richinnovations Technologies Pvt. Ltd.</td>\n",
       "      <td>Closing Date: 21,Mar 2025</td>\n",
       "      <td>2025-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUSINESS DEVELOPMENT EXECUTIVE</td>\n",
       "      <td>Reflections Info Systems Pvt Ltd</td>\n",
       "      <td>Closing Date: 14,Mar 2025</td>\n",
       "      <td>2025-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOLUTION ARCHITECT (DIGITAL/ DATA PRACTICE)</td>\n",
       "      <td>Reflections Info Systems Pvt Ltd</td>\n",
       "      <td>Closing Date: 14,Mar 2025</td>\n",
       "      <td>2025-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QA PRACTICE LEAD (HANDSON, DOMAIN SPECIALIST)</td>\n",
       "      <td>LEADER IT PRIVATE LIMITED</td>\n",
       "      <td>Closing Date: 15,Mar 2025</td>\n",
       "      <td>2025-03-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Job Titles  \\\n",
       "0                                                  \n",
       "1                    SENIOR SHAREPOINT DEVELOPER   \n",
       "2                 BUSINESS DEVELOPMENT EXECUTIVE   \n",
       "3    SOLUTION ARCHITECT (DIGITAL/ DATA PRACTICE)   \n",
       "4  QA PRACTICE LEAD (HANDSON, DOMAIN SPECIALIST)   \n",
       "\n",
       "                                    Firms               Closing Date  \\\n",
       "0        Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
       "1  Richinnovations Technologies Pvt. Ltd.  Closing Date: 21,Mar 2025   \n",
       "2        Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
       "3        Reflections Info Systems Pvt Ltd  Closing Date: 14,Mar 2025   \n",
       "4               LEADER IT PRIVATE LIMITED  Closing Date: 15,Mar 2025   \n",
       "\n",
       "  Posted Date  \n",
       "0  2025-03-07  \n",
       "1  2025-03-07  \n",
       "2  2025-03-07  \n",
       "3  2025-03-07  \n",
       "4  2025-03-07  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "662bff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Job Titles    20 non-null     object        \n",
      " 1   Firms         20 non-null     object        \n",
      " 2   Closing Date  20 non-null     object        \n",
      " 3   Posted Date   20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 772.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597bdb0",
   "metadata": {},
   "source": [
    "# Trial 2-Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e200a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "web='https://www.amazon.in/s?k=laptop&crid=VG7IMBWSEX9X&sprefix=laptop%2Caps%2C260&ref=nb_sb_noss_2'\n",
    "path='C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "driver=webdriver.Chrome(path)\n",
    "driver.get(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea926d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "web = 'https://www.amazon.in/s?k=laptop'\n",
    "path = 'C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "driver.get(web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd62f8",
   "metadata": {},
   "source": [
    "Amazon web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302ba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop=[]\n",
    "price=[]\n",
    "last_month_purchase=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f84bef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Setup\n",
    "web = 'https://www.amazon.in/s?k=laptop'\n",
    "path = 'C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\")\n",
    "\n",
    "# Start driver\n",
    "driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "\n",
    "laptops = []\n",
    "\n",
    "# Loop through 10 pages\n",
    "for page in range(1, 11):\n",
    "    driver.get(f'{web}&page={page}')\n",
    "    time.sleep(2)  # Pause to avoid getting blocked\n",
    "\n",
    "    items = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")\n",
    "\n",
    "    for item in items:\n",
    "        try:\n",
    "            name = item.find_element(By.XPATH, \".//h2\").get_attribute(\"aria-label\")\n",
    "    \n",
    "        except:\n",
    "            name = 'N/A'\n",
    "\n",
    "        try:\n",
    "            price = item.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        except:\n",
    "            price = 'N/A'\n",
    "\n",
    "        try:\n",
    "            last_month = item.find_element(By.XPATH, \".//span[@class='a-size-base a-color-secondary']\").text\n",
    "        except:\n",
    "            last_month = 'N/A'\n",
    "\n",
    "        laptops.append([name, price, last_month])\n",
    "\n",
    "# Save to CSV\n",
    "#with open('amazon_laptops.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    " #   writer = csv.writer(file)\n",
    " #   writer.writerow([\"Name\", \"Price\", \"Rating\"])\n",
    "  #  writer.writerows(laptops)\n",
    "\n",
    "#print(\"Scraping complete. Data saved to amazon_laptops.csv\")\n",
    "\n",
    "# Close driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fd3bd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>last_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sponsored Ad - HP 15s, 12th Gen Intel Core i5-...</td>\n",
       "      <td>45,022</td>\n",
       "      <td>200+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sponsored Ad - HP 15, 13th Gen Intel Core i5-1...</td>\n",
       "      <td>57,990</td>\n",
       "      <td>M.R.P:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acer Aspire Lite, AMD Ryzen 5-5625U, 16GB RAM,...</td>\n",
       "      <td>34,490</td>\n",
       "      <td>1K+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo V15 G4 AMD Athlon Silver 7120U Laptop 8...</td>\n",
       "      <td>24,990</td>\n",
       "      <td>50+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple MacBook Air Laptop: Apple M1 chip, 13.3-...</td>\n",
       "      <td>65,990</td>\n",
       "      <td>1K+ bought in past month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name   Price  \\\n",
       "0  Sponsored Ad - HP 15s, 12th Gen Intel Core i5-...  45,022   \n",
       "1  Sponsored Ad - HP 15, 13th Gen Intel Core i5-1...  57,990   \n",
       "2  Acer Aspire Lite, AMD Ryzen 5-5625U, 16GB RAM,...  34,490   \n",
       "3  Lenovo V15 G4 AMD Athlon Silver 7120U Laptop 8...  24,990   \n",
       "4  Apple MacBook Air Laptop: Apple M1 chip, 13.3-...  65,990   \n",
       "\n",
       "                  last_month  \n",
       "0  200+ bought in past month  \n",
       "1                     M.R.P:  \n",
       "2   1K+ bought in past month  \n",
       "3   50+ bought in past month  \n",
       "4   1K+ bought in past month  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(laptops, columns=['Name', 'Price', 'last_month'])\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6dad03",
   "metadata": {},
   "source": [
    "# Trial 3-BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4513ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "web='https://www.bbc.com/'\n",
    "path='C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "driver=webdriver.Chrome(path)\n",
    "driver.get(web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e09a92",
   "metadata": {},
   "source": [
    "BBC Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f1ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIVE\n",
      "START\n",
      "LIVE\n",
      "Video quality\n",
      "Low\n",
      "Medium\n",
      "Highest available\n",
      "Automatically selects the best quality available\n",
      "00:00\n",
      "Try Again\n",
      "0:58\n",
      "Watch: Rodrigo Duterte questions ICC warrant for his arrest\n",
      "Philippine police have arrested former President Rodrigo Duterte after the International Criminal Court (ICC) issued a warrant accusing him of crimes against humanity over his deadly \"war on drugs\".\n",
      "The 79-year-old was taken into police custody shortly after his arrival at Manila airport from Hong Kong.\n",
      "He has offered no apologies for his brutal anti-drugs crackdown, which saw thousands of people killed when he was president of the South East Asian nation from 2016 to 2022, and mayor of Davao city before that.\n",
      "Upon his arrest, he questioned the basis for the warrant, asking: \"What crime [have] I committed?\"\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "\n",
    "# WebDriver setup\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"https://www.bbc.com/\")\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Click the \"News\" button\n",
    "news_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/nav/section/nav/ul/li[2]/div/a')))\n",
    "news_button.click()\n",
    "\n",
    "# Scroll and click the \"Article\" button\n",
    "article_button = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-content\"]/article/section[1]/div/div/div[1]/div/div/div/a/div/div[2]/div[1]/div/h2')))\n",
    "\n",
    "# Scroll into view and click\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", article_button)\n",
    "driver.execute_script(\"arguments[0].click();\", article_button)\n",
    "\n",
    "# Extract article text after the page loads\n",
    "article_body = wait.until(EC.presence_of_element_located((By.TAG_NAME, 'article')))\n",
    "article_text = article_body.text\n",
    "\n",
    "print(article_text)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63dc57a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footage aired on local television showed him walking out of the airport using a cane. Authorities say he is in \"good health\" and is being cared for by government doctors.\n",
      "\"What is my sin? I did everything in my time for peace and a peaceful life for the Filipino people,\" he told a cheering crowd of Filipino expatriates before leaving Hong Kong.\n",
      "A video posted by his daughter, Veronica Duterte, showed Duterte in custody in a lounge at Manila's Villamor Air Base. In it, he can be heard questioning the reason for his arrest.\n",
      "\"What is the law and what is the crime that I committed? I was brought here not of my own volition, it is somebody else's. You have to answer now for the deprivation of liberty.\"\n",
      "A previous UN report found that most victims were young poor urban males and that police, who do not need search or arrest warrants to conduct house raids, systematically forced suspects to make self-incriminating statements or risk facing lethal force.\n",
      "Critics said the campaign targeted street-level pushers from the urban poor and failed to catch big-time drug lords. Many families also claimed that the victims - their sons, brothers or husbands - were simply in the wrong place at the wrong time.\n",
      "Investigations in parliament pointed to a shadowy \"death squad\" of bounty hunters targeting drug suspects. Duterte has denied the allegations of abuse.\n",
      "\"Do not question my policies because I offer no apologies, no excuses. I did what I had to do, and whether or not you believe it... I did it for my country,\" Duterte told a parliament investigation in October.\n",
      "\"I hate drugs, make no mistake about it.\"\n",
      "The ICC first took note of the alleged abuses in 2016 and started its investigation in 2021. It covered cases from November 2011, when Duterte was mayor of Davao, to March 2019, before the Philippines withdrew from the ICC.\n",
      "Duterte remains popular in the Philippines as he is the country's first leader from Mindanao, a region south of Manila, where many feel marginalised by the leaders in the capital.\n",
      "He often speaks in Cebuano, the regional language, not Tagalog, which is more widely-spoken in Manila and northern regions.\n",
      "His populist rhetoric and blunt statements earned him the moniker \"Donald Trump of the East\". He has called Russian President Vladimir Putin his \"idol\" and under his administration, the Philippines' pivoted their foreign policy to China away from the US, its long-standing ally.\n",
      "His daughter and political heir, Sara Duterte, is the Philippines' current vice-president and is tipped as a potential presidential candidate in 2028.\n",
      "In recent months, the Duterte family's alliance with incumbent President Ferdinand Marcos unraveled spectacularly before the public view, soon after Marcos and Sara Duterte won the 2022 elections by a landslide.\n",
      "Marcos initially refused to co-operate with the ICC investigation, but as his relationship with the Duterte family deteriorated, he changed his stance, and later indicated that the Philippines would co-operate.\n",
      "It is not clear yet whether Marcos would go as far as extraditing the former president to stand trial in The Hague.\n",
      "Additional reporting by Virma Simonette in Manila\n",
      "Officials in central Manila are offering two pesos for every five mosquitoes captured, dead or alive.\n",
      "Manila says it will file a formal protest over the latest incident in the South China Sea.\n",
      "It comes amid a bitter feud between President Ferdinand Marcos Jr and his vice-president Sara Duterte.\n",
      "International disaster relief charity ShelterBox responds after six major storms in the Philippines.\n",
      "Analisa Josefa Corr has been accused of assaulting a fellow passenger while intoxicated.\n",
      "Copyright 2025 BBC. All rights reserved.  The BBC is not responsible for the content of external sites. Read about our approach to external linking.\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "\n",
    "# WebDriver setup\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"https://www.bbc.com/\")\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Click the \"News\" button\n",
    "news_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/nav/section/nav/ul/li[2]/div/a')))\n",
    "news_button.click()\n",
    "\n",
    "# Scroll and click the \"Article\" button\n",
    "article_button = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-content\"]/article/section[1]/div/div/div[1]/div/div/div/a/div/div[2]/div[1]/div/h2')))\n",
    "\n",
    "# Scroll into view and click\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", article_button)\n",
    "driver.execute_script(\"arguments[0].click();\", article_button)\n",
    "\n",
    "# Extract all paragraphs inside the article\n",
    "paragraphs = wait.until(EC.presence_of_all_elements_located((By.TAG_NAME, 'p')))\n",
    "article_text = \"\\n\".join([p.text for p in paragraphs if p.text.strip() != \"\"])  # Combine paragraphs, skip empty ones\n",
    "\n",
    "print(article_text)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0ae959e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4560dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Footage aired on local television showed him walking out of the airport using a cane. Authorities say he is in \"good health\" and is being cared for by government doctors.', '\"What is my sin? I did everything in my time for peace and a peaceful life for the Filipino people,\" he told a cheering crowd of Filipino expatriates before leaving Hong Kong.', \"A video posted by his daughter, Veronica Duterte, showed Duterte in custody in a lounge at Manila's Villamor Air Base. In it, he can be heard questioning the reason for his arrest.\"]\n"
     ]
    }
   ],
   "source": [
    "first_paragraph = article_text.split('\\n')[0:3]  # Splits on newlines and takes the first one\n",
    "print(first_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed849a",
   "metadata": {},
   "source": [
    "# TRIAL 4-QUORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63b63ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Extracted Answer:\n",
      "\n",
      "Something went wrong. Wait a moment and try again.\n",
      "Total GDP of the 5 southern states is 1.21 Trillion for a population of ~24 crores. India’s GDP is 3 Trillion for a population of 130 crores. So removing the 5 states, the GDP of Rest of India will be 1.8 Trillion for 110 crore people.\n",
      "Hi,\n",
      "For stock market beginners and traders, the Tradee Telegram channel is the perfect choice. It provides simple strategies, live trading ideas, and daily market updates — making it easy to learn, grow, and trade confidently.\n",
      "Join here: https://telegram.me/tradee\n",
      "Thanks to Tradee, I was able to achieve these profits!\n",
      "Join here: https://telegram.me/tradee\n",
      "If we consider this situation hypothetically, South India would be a $1 trillion economy (roughly) with a population of about 25 crores. It will be among the 20 largest economies in the world. It will also be the among the five most populated countries in the world. It won't exactly be a prosperous country but it would be comparable to Indonesia in terms of GDP, GDP per capita as well as overall population.\n",
      "It looks decent on papers. However, in reality there are various factors that can affect the country. First, there should be no hostility between South India and the rest of India. The chang\n",
      "If we consider this situation hypothetically, South India would be a $1 trillion economy (roughly) with a population of about 25 crores. It will be among the 20 largest economies in the world. It will also be the among the five most populated countries in the world. It won't exactly be a prosperous country but it would be comparable to Indonesia in terms of GDP, GDP per capita as well as overall population.\n",
      "It looks decent on papers. However, in reality there are various factors that can affect the country. First, there should be no hostility between South India and the rest of India. The change should be seen only as an administrative change. All businesses, trade, movement of labour/people should not be restricted. In short, it should be like European countries with minimal restrictions. Second, there should be no security issues, violence or border issues. Any one of these can severely affect the economy of both regions. Third, the states of South India are extremely diverse and each state should be given full autonomy to avoid internal issues. All these are easier said than done in the Indian subcontinent.\n",
      "Our subcontinent is not ready for the EU type administration yet. However, we can start with the decentralisation of powers to the respective states.\n",
      "Mostly likely South Indian Infrastructure will be as good as China by 2030 and we will probably be a developed country with a 2.5 Trillion dollar economy. Right now our taxes are siphoned off to the north For every 1 Rupee collected we get back around 40 paise from the union government. But, if that money is invested back in the South a massive transformation of road transport, railways, education and helath care is possible. We could use the money saved to set up many world class universities. Private industry will take care of the rest. Our population growth will stabilise, meaning we will p\n",
      "Mostly likely South Indian Infrastructure will be as good as China by 2030 and we will probably be a developed country with a 2.5 Trillion dollar economy. Right now our taxes are siphoned off to the north For every 1 Rupee collected we get back around 40 paise from the union government. But, if that money is invested back in the South a massive transformation of road transport, railways, education and helath care is possible. We could use the money saved to set up many world class universities. Private industry will take care of the rest. Our population growth will stabilise, meaning we will probably become self sufficient in resources soon. Kerala, Coastal Karnataka and Coastal Andhra are more than 90% literate, Tamil Nadu is near 80%, Karnataka at 75%, with Telangana lagging behind at 66%. In 10 years 100%literacy is possible if North Karnataka and Telangana catch up. The United States of South India will definitely an industrial and economic superpower because of the availability of highly skilled labour, natural resources and access to the Indian ocean for trade to the east, west and south. 500 years ago the Vijayanagar Empire did just that. You can read the accounts of foreign travellers like Nicola Conti of Hampi to get a perspective of how prosperous the south could be.\n",
      "We could establish excellent trade relationships with enemy countries like China and Pakistan too. We could for example ship Hyundai cars from Chennai or Volvo buses from Bangalore to Karachi port via Mangalore. Similarly drugs manufactured in Hyderabad or cotton textiles from Tiruppur could be sold in Pakistan and afghanistan easily. Ships built in Vishakapatanam and Steel from steel plants in Bellary could be sold in China. The possibilities are limitless.\n",
      "The only danger is that North India will get greedy and send their Army to invade just like North Indian kingdoms have been doing for the last 2500 years or so. The truth is today they treat us like a colony. We have lost our self respect and our will to resist. They impose Hindi on us against our will. They loot us through taxes. They insist that we reduce our population growth while encouraging population growth in the north. They then punish us for achieving our targets as top performers in population control, literacy, healthcare etc by reducing funds. After the next delimitation of Lok Sabha seats we will have even less say in the central government. Eventually we will probably have 10 seats like the North East. Ask anyone from newly formed Andhra if any of the central government’s promises have been fulfilled. In fact you will find resentment all over in Karnataka, Tamil Nadu and Kerala too. The\n",
      "The South needs to wake up to reality.\n",
      "I am a North Indian. Yet, I will support secession of South India from India. But, ONLY on ONE CONDITION. North India (and other remaining parts of India) should be allowed to join the newly formed country called, South India!!! Will come back to this point later.\n",
      "Let me narrate a small story from my school books. A farmer had four sons. They were hardworking but used to fight always with one-another. Worried due to his old age as to what would happen to them after his death, the farmer thought of a plan. He collected many sticks and prepared a bundle of these sticks. He asked his sons, individ\n",
      "I am a North Indian. Yet, I will support secession of South India from India. But, ONLY on ONE CONDITION. North India (and other remaining parts of India) should be allowed to join the newly formed country called, South India!!! Will come back to this point later.\n",
      "Let me narrate a small story from my school books. A farmer had four sons. They were hardworking but used to fight always with one-another. Worried due to his old age as to what would happen to them after his death, the farmer thought of a plan. He collected many sticks and prepared a bundle of these sticks. He asked his sons, individually, to break that bundle of sticks. But, nobody could do it. Then he untied the bundle and gave individual single sticks to his sons. This time, all his sons were able to break the individual sticks. Moral of the story? There is strength in unity. It is easier for a person to break several sticks individually, but it is impossible for him to break several sticks tied together in a bundle.\n",
      "Do you consider USA to be a powerful nation, if not the most powerful nation? Now, divide USA into 50 states and make them all independent. Are the individual states still so powerful? Okay, just divide USA in two parts only. Will the individual parts be still as powerful?\n",
      "What about doing the same thing to China? Do you get a different result?\n",
      "What happened to USSR when it broke apart into 15 individual states? Is Russia or any of the other 14 states that came out of USSR (such as Estonia, Latvia, Lithuania, Kazakhstan, Kyrgyzstan, Tajikistan, etc.) individually as powerful as the original USSR? In fact, even Russia, which has about 75% of the total area of USSR, is not as mighty and is a pale shadow of its past.\n",
      "I will not even talk about Singapore or Israel that are quite small countries but yet thrive. But, if small countries like Nepal and Bhutan can survive as independent nations, even though they are land-locked and lack many resources, there is no question as to why North India or South India cannot survive separately. But, the question is at what cost? Whether as a powerful nation, or as a minion?\n",
      "Whatever weaknesses India may have, we are still a strong nation in the comity of nations. Precisely, 2018 years back, in 1 AD, share of India in world GDP was 32%. Even in 1500 AD, it was 24.4%. And, in 1870 AD, India’s share in world GDP was still 12.2%. [Please see: Share of world GDP throughout history]\n",
      "[Image source: Share of world GDP throughout history by Dave Drabble - Infogram]\n",
      "However, in 2017, India’s share in world GDP is expected to be only about 3.2%. [See: List of countries by GDP (nominal)]\n",
      "So, India has been a powerful nation throughout the history, but due to foreign rulers for more than a 1000 years, our might drastically reduced due to oppressive rulers.\n",
      "But, we can make India a great nation, again. Together.\n",
      "Together we win. If you divide India, all of us lose to varying degrees.\n",
      "South India is the most precious jewel that India cannot afford to lose. India will not be what it is today without South India.\n",
      "Now, let me come back to what I said in the beginning of this answer. Why did I say that let South India secede from India, and then allow North India and other parts to join this newly created South India nation?\n",
      "Well, please forgive me for saying that South Indian citizens are, GENERALLY SPEAKING, more principled and more work-oriented than North Indian citizens. They have better value system. Before North Indians start blasting me off, let me refer to the qualifying words: GENERALLY SPEAKING. Moreover, I am myself a North Indian. Further, one exception to this rule is that politicians in both parts of India are one and the same.\n",
      "I have made the above statement from my personal experiences over last 40-50 years. If you don’t agree, please ignore it.\n",
      "I have been to all states in India, except Goa and north-east. I have lived in South India for a reasonable period of time and have travelled extensively.\n",
      "In recent past, I have been frequently going to Bangalore (IIM and NLSIU) and Hyderabad (NPA). Have visited other areas in South for investigation while being in CBI. My experiences have always been comparatively more pleasant.\n",
      "In my college days, as well as during my service period, I always had the best experience when I got to work with some South Indian teachers / officers. Of course, this is in comparative terms, so please do not consider that I am denigrating anyone.\n",
      "I am not saying that South India is “white” and North India is “black”. They are somewhere in between, different shades of grey. But, definitely, my experience has been that South India has a better shade of grey. North is generally in mess with too many problems, of its own creation.\n",
      "I hope my North Indian friends would forgive me. But, what I have mentioned is my own personal experience. You may have your own personal experience which may be contrary to this. However, I can write about my own experiences or the general experiences as I see from media.\n",
      "So, I feel that if South is to secede from India, let them allow other states (including North) to join them in the new nation. This would mean India as a whole becoming a new nation with a NEW CONSTITUTION, though it may be named as South India now or something else. We will get an opportunity to rewrite the rules of our new system and make the new system better.\n",
      "Lastly, let me come to the issue that has given rise to the demand of South to secede from India. It has been triggered mainly due to the Terms of Reference for the 15th Finance Commission. The census of 2011 has been made the reference point for deciding the distribution of Central taxes among states, instead of the previous benchmark of 1971 census (which was frozen in previous Finance Commissions), though the Finance Minister Arun Jaitley claims that there is also another reference “efforts and progress made in moving towards replacement rate of population growth” which would counterbalance it.[1] The above criterion is likely to adversely affect the Southern states which have better track record in controlling population.\n",
      "Frankly speaking, I see no reason as to why the base should be shifted from 1971 census to 2011 census. Why should the Southern states suffer merely because they have controlled their population in a better way? In fact, they should be rewarded for this. And, the Northern states which could not control population effectively should be penalised by giving them less share of taxes.\n",
      "If I were Narendra Modi, I would immediately change the Terms of Reference for the 15th Finance Commission back to the 1971 census. And, in fact, I’ll also give extra incentives in the form of enhanced share in Central taxes for those states which have better controlled the population. Such measure will persuade or rather compel the Northern states to do better on the population front. In fact, this would be a good opportunity to try to control population.\n",
      "To end, let me point out that the question (which is answered here) is a hypothetical issue. So, please do not read too much in what I have written. South will never secede from India. India will always remain united. The demands for secession, etc., are the imaginary creations of politicians who always benefit by dividing people on different issues, religion, caste, language, and now region. As if the existing divisions on the lines of religion, caste and language were not sufficient, that the politicians want now to create new divisions on regional basis. Hell with such divisive politicians.\n",
      "INDIA SHALL REMAIN UNITED, COME WHAT MAY.\n",
      "Footnotes\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "web = 'https://www.quora.com/How-prosperous-will-South-India-be-globally-if-it-breaks-away-from-the-Indian-Union?topAns=285226767'\n",
    "path = 'C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "\n",
    "# Allow time for content to load and scroll\n",
    "time.sleep(5)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the full HTML after JavaScript loads\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Try to extract all visible paragraphs (you can refine this further)\n",
    "all_paras = soup.find_all('p')\n",
    "print(\"🔹 Extracted Answer:\\n\")\n",
    "for para in all_paras:\n",
    "    print(para.text)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18403e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6015b772",
   "metadata": {},
   "source": [
    "# TRIAL-5 IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ecde065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page title: Top 250 TV shows\n",
      "Number of rows found: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "path = 'C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "url = \"https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# DO NOT use headless\n",
    "# options.add_argument(\"--headless\")  \n",
    "\n",
    "driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "driver.get(url)\n",
    "\n",
    "# Optional: scroll to bottom to trigger lazy loading\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"Page title:\", driver.title)\n",
    "\n",
    "# Now try to grab rows\n",
    "rows = driver.find_elements(By.CSS_SELECTOR, 'table.chart.full-width tbody tr')\n",
    "print(\"Number of rows found:\", len(rows))  # Should be ~250\n",
    "\n",
    "top_shows = []\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        title_col = row.find_element(By.CSS_SELECTOR, 'td.titleColumn')\n",
    "        rating_col = row.find_element(By.CSS_SELECTOR, 'td.imdbRating')\n",
    "\n",
    "        title = title_col.find_element(By.TAG_NAME, 'a').text\n",
    "        year = title_col.find_element(By.TAG_NAME, 'span').text.strip('()')\n",
    "        rank = title_col.text.split('.')[0].strip()\n",
    "        rating = rating_col.find_element(By.TAG_NAME, 'strong').text\n",
    "\n",
    "        top_shows.append({\n",
    "            'rank': int(rank),\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'rating': float(rating)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing row:\", e)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(top_shows)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ab6d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests-html in c:\\users\\kunni\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (2.32.3)\n",
      "Requirement already satisfied: pyquery in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (2.0.1)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (1.5.1)\n",
      "Requirement already satisfied: parse in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (1.20.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (0.0.2)\n",
      "Requirement already satisfied: w3lib in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (1.21.0)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests-html) (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2023.7.22)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (6.0.0)\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Obtaining dependency information for pyee<12.0.0,>=11.0.0 from https://files.pythonhosted.org/packages/db/99/7e80837f60b13227f03334e3b0537d650dea2c0cea44c543b0a2e719a48f/pyee-11.1.1-py3-none-any.whl.metadata\n",
      "  Using cached pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.4)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.16)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from bs4->requests-html) (4.12.2)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (4.9.2)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests->requests-html) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.4)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from w3lib->requests-html) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kunni\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (2.4)\n",
      "Using cached pyee-11.1.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyee\n",
      "  Attempting uninstall: pyee\n",
      "    Found existing installation: pyee 12.0.0\n",
      "    Uninstalling pyee-12.0.0:\n",
      "      Successfully uninstalled pyee-12.0.0\n",
      "Successfully installed pyee-11.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "playwright 1.48.0 requires pyee==12.0.0, but you have pyee 11.1.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43c87b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page title: IMDb Top 250 movies\n",
      "Number of rows found: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "path = 'C:/UNNI/Analytics/data/Udemy-web-scraping/chromedriver-win64/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"Page title:\", driver.title)\n",
    "driver.save_screenshot(\"imdb_movies_screenshot.png\")\n",
    "\n",
    "rows = driver.find_elements(By.CSS_SELECTOR, 'table.chart.full-width tbody tr')\n",
    "print(\"Number of rows found:\", len(rows))\n",
    "\n",
    "top_movies = []\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        title_col = row.find_element(By.CSS_SELECTOR, 'td.titleColumn')\n",
    "        rating_col = row.find_element(By.CSS_SELECTOR, 'td.imdbRating')\n",
    "\n",
    "        title = title_col.find_element(By.TAG_NAME, 'a').text\n",
    "        year = title_col.find_element(By.TAG_NAME, 'span').text.strip('()')\n",
    "        rank = title_col.text.split('.')[0].strip()\n",
    "        rating = rating_col.find_element(By.TAG_NAME, 'strong').text\n",
    "\n",
    "        top_movies.append({\n",
    "            'rank': int(rank),\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'rating': float(rating)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(top_movies)\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da990e",
   "metadata": {},
   "source": [
    "# Trial-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19e082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Team  Year Wins Losses OT Losses  Win % Goals For  \\\n",
      "0       Boston Bruins  1990   44     24             0.55       299   \n",
      "1      Buffalo Sabres  1990   31     30            0.388       292   \n",
      "2      Calgary Flames  1990   46     26            0.575       344   \n",
      "3  Chicago Blackhawks  1990   49     23            0.613       284   \n",
      "4   Detroit Red Wings  1990   34     38            0.425       273   \n",
      "\n",
      "  Goals Against Goal Diff  \n",
      "0           264        35  \n",
      "1           278        14  \n",
      "2           263        81  \n",
      "3           211        73  \n",
      "4           298       -25  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Loop through multiple pages\n",
    "for page in range(1, 5):  # You can increase the range as needed\n",
    "    url = f\"https://www.scrapethissite.com/pages/forms/?page_num={page}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find each row (team) on the page\n",
    "    rows = soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "    for row in rows:\n",
    "        team_name = row.find(\"td\", class_=\"name\").text.strip()\n",
    "        year = row.find(\"td\", class_=\"year\").text.strip()\n",
    "        wins = row.find(\"td\", class_=\"wins\").text.strip()\n",
    "        losses = row.find(\"td\", class_=\"losses\").text.strip()\n",
    "        ot_losses = row.find(\"td\", class_=\"ot-losses\").text.strip()\n",
    "        win_pct = row.find(\"td\", class_=\"pct\").text.strip()\n",
    "        goals_for = row.find(\"td\", class_=\"gf\").text.strip()\n",
    "        goals_against = row.find(\"td\", class_=\"ga\").text.strip()\n",
    "        diff = row.find(\"td\", class_=\"diff\").text.strip()\n",
    "\n",
    "        all_data.append({\n",
    "            \"Team\": team_name,\n",
    "            \"Year\": year,\n",
    "            \"Wins\": wins,\n",
    "            \"Losses\": losses,\n",
    "            \"OT Losses\": ot_losses,\n",
    "            \"Win %\": win_pct,\n",
    "            \"Goals For\": goals_for,\n",
    "            \"Goals Against\": goals_against,\n",
    "            \"Goal Diff\": diff\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127fa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
